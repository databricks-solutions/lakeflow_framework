{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./initialize\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Schemas and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {staging_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_schema}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {gold_schema}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {staging_schema}.{staging_volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE DATA FROM VOLUME IF EXISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_deletion(path, max_wait=60, sleep_interval=2):\n",
    "    \"\"\"\n",
    "    Wait until the specified path is deleted or timeout occurs.\n",
    "\n",
    "    :param path: Path to check for existence\n",
    "    :param max_wait: Maximum wait time in seconds\n",
    "    :param sleep_interval: Time to wait between checks\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait:\n",
    "      try:\n",
    "        dbutils.fs.ls(path)\n",
    "        time.sleep(sleep_interval)\n",
    "      except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "          print(f\"Deletion confirmed: {path} does not exist.\")\n",
    "          return True\n",
    "        else:\n",
    "          raise e\n",
    "    \n",
    "    print(f\"Warning: Timeout reached while waiting for {path} to be deleted.\")\n",
    "    return False\n",
    "\n",
    "# Trigger deletion\n",
    "dbutils.fs.rm(f\"{volume_root_file_path}\", True)\n",
    "\n",
    "# Wait until fully deleted\n",
    "wait_for_deletion(volume_root_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORTING TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = (\n",
    "  spark.sql(f\"SELECT * FROM {sample_source_schema}.region\").withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "  .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "  .save(f\"{volume_root_file_path}/region/region_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "nation_items_df = (\n",
    "  spark.sql(f\"SELECT * FROM {sample_source_schema}.nation\").withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "  .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "  .save(f\"{volume_root_file_path}/nation1/nation_{current_time_str}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOMER TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = (\n",
    "    spark.sql(f\"SELECT c_custkey as customer_id, c_name as name, c_acctbal as acctbal, c_mktsegment as mktseg FROM {sample_source_schema}.customer\").dropDuplicates().withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/customer/customer_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "customer_address_df = (\n",
    "    spark.sql(f\"SELECT c_custkey as customer_id, c_address as address, c_nationkey as nat_id FROM {sample_source_schema}.customer\").dropDuplicates().withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/customer_address/customer_address_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "customer_phone_df = (\n",
    "    spark.sql(f\"SELECT c_custkey as customer_id, 'M' as type, c_phone as phone FROM {sample_source_schema}.customer\").dropDuplicates().withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/customer_phone/customer_phone{current_time_str}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPLIER TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_df = (\n",
    "    spark.sql(f\"SELECT s_suppkey as supplier_id, s_name as name, s_acctbal as acctbal, s_comment as comment FROM {sample_source_schema}.supplier\").dropDuplicates().withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/supplier/supplier_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "supplier_address_df = (\n",
    "    spark.sql(f\"SELECT s_suppkey as supplier_id, s_address as address, s_nationkey as nat_id FROM {sample_source_schema}.supplier\").dropDuplicates().withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/supplier_address/supplier_address_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "supplier_phone_df = (\n",
    "    spark.sql(f\"SELECT s_suppkey as supplier_id, s_phone as phone FROM {sample_source_schema}.supplier\").dropDuplicates().withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/supplier_phone/supplier_phone_{current_time_str}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORDERS TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = (\n",
    "  spark.sql(f\"SELECT * FROM {sample_source_schema}.orders\").withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/orders/orders_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "line_items_df = (\n",
    "  spark.sql(f\"SELECT * FROM {sample_source_schema}.lineitem\").withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/lineitem/lineitem_{current_time_str}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = (\n",
    "  spark.sql(f\"SELECT * FROM {sample_source_schema}.part\").withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/part/part_{current_time_str}.csv\")\n",
    ")\n",
    "\n",
    "partsupp_df = (\n",
    "  spark.sql(f\"SELECT * FROM {sample_source_schema}.partsupp\").withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "    .write.format(\"csv\").mode(\"overwrite\").options(**writer_options)\n",
    "    .save(f\"{volume_root_file_path}/partssupp/partssupp_{current_time_str}.csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
